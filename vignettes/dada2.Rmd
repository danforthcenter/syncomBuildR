---
title: "DADA2 For SINC using AWS"
author: "Josh Sumner, DDPSC Data Science Core Facility"
output: 
  html_document:
    toc: true
    number_sections: false
    code_folding: show
date: "2024-09-10"
vignette: >
  %\VignetteIndexEntry{dada2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

```{r, eval=TRUE}
library(data.table)
library(ggplot2)
```

## Introduction

Scale-Out Computing on AWS (SOCA) is the replacement for our local compute cluster that was accessible
through stargate. There are docs that will cover the basics of using SOCA/S3 that Noah and Josh R put
together as they were preparing the center to migrate to AWS. Those can be found [here](https://docs.google.com/document/d/1NdfYnbF7iT6VDSrKsCwh57z5aYbjJHsHGCMBzWVVGyA/edit#heading=h.ifc0b6hs1hz7). Those will cover most of what you need for general information and the point of this document will
be to go over the steps that I did to run `dada2` on the field 2021 and field 2022 samples (together)
in case there are parts of this which are useful to you in the future.

## Using SOCA

Note that I am writing these as someone using linux and without cyberduck/other filesharing GUIs.
There may be times that it is easier for you to do something different depending on your OS/expertise.

### Accessing SOCA

First you need to be able to access SOCA (probably from command line). To do that you need to follow
the instructions [here](https://soca.datasci.danforthcenter.org/ssh) to get a `YOURNAME_soca_privatekey.pem`
file. Once you have that file you can use it to ssh into the SOCA head node. Generally it will be easiest
to make an alias for that similar to below.

```
cd
nano .bashrc # or vim or emacs or however you want to edit the file
# then add:
# alias soca='ssh -i /path/to/soca_privatekey.pem username@3.140.76.124'
# for instance mine is:
# alias soca='ssh -i /home/josh/Desktop/jsumner_soca_privatekey.pem jsumner@3.140.76.124'
```

Now restart bash (open a new terminal) and run `soca` which should take you to the head node.

### Job Setup on the head node

On the head node there is a job template setup tool that you can use to make an empty job template with
all default settings.

```
[jsumner@ip-10-0-0-38 ~]$ job-template myjob.sh
Created job file myjob.sh.
Edit the job file to add/edit parameters and job information.
Submit the job using the command: qsub <jobfile>, or for interactive jobs: qsub -I <jobfile>
Additional parameters are documented at
https://awslabs.github.io/scale-out-computing-on-aws/tutorials/integration-ec2-job-parameters/
```

Note that for our purposes it makes sense to start off by running a smallish machine interactively so
that you can make sure that you have installed and can load all of the R packages you'll need before
spinning up a larger machine.

That job file has several PBS settings, although [more are documented online](https://awslabs.github.io/scale-out-computing-on-aws/tutorials/integration-ec2-job-parameters/#scratch_size). Other than the PBS settings and variables the rest is a normal .sh script.

```
[jsumner@ip-10-0-0-38 ~]$ cat myjob.sh
#!/bin/bash
## BEGIN PBS SETTINGS: Note PBS lines MUST start with #
#PBS -N jobname
#PBS -V -j oe -o jobname.qlog
#PBS -P bart
#PBS -q normal
#PBS -l nodes=1
#PBS -l instance_type=t3.nano
#PBS -l system_metrics=True
## END PBS SETTINGS
## BEGIN JOB

## END JOB
```

### Job Customization

Next you'll need to customize the job to work for your specific needs.

#### PBS settings

First we name the job, here I'm naming it `run_dada2`. This is just useful for tracking the job in the
queue. 

Next we specify where to write logs, I am writing them to `run_dada2.qlog` so that I know they are from
the `run_dada2.sh` job and related to the `run_dada2.R` file that we'll also write.

Then we specify that we are running this as part of the bart lab. I do not change the queue or the number
of nodes from the default.

We do change the instance type to a larger machine. `m5.16xlarge` is a 32 core x 256 GiB machine, but
a `c5.24xlarge` instance might be better (48 core x 192 GiB machine) since it has more cores and is
slightly less expensive per hour. I was using the `m5.16xlarge` out of an abundance of caution for
memory while testing and one of those tests happened to be the version that worked first.
It is very unlikely that 192 is too little memory for this job though, so you should explore options.

One type of memory that we do need to worry about is the `/scratch/` memory. When you start a compute
instance that is a machine that exists while your job runs, then when your job finishes the machine goes
away. While the machine runs there is a directory called `/scratch/` where you can write files that will
not incur the (rather expensive) storage costs on EBS (the head node you access with your `soca` ssh
alias). The problem is that the `/scratch/` directory WILL DISAPPEAR when your job concludes, so you'll
need to copy things to someplace else before ending the job. Additionally, `dada2` wants to read fastq
files then write a bunch of filtered fastq files to someplace else. The `/scratch/` folder is the most
obvious place to write those files since they may change based on different parameters you set within
the R script and we don't want to store them forever since they are just intermediate data. By default
there is very little disk memory in `/scratch/` so we specify that we want a machine with 25Gb of storage
there. Similar to the machine memory, 25Gb is a very generous estimate, but it is something where erring
on the side of caution can save a lot of headaches. This folder will have filtered fastq files, some
taxonomy files, and all your Rdata output in it at the end of your job, better to not have it fail at
the very end due to memory limits. To specify `/scratch/` storage we add a `scratch_size=25` line in our
PBS settings.

The complete setup chunk looks like this:

```
#!/bin/bash
## BEGIN PBS SETTINGS: Note PBS lines MUST start with #
#PBS -N run_dada2
#PBS -V -j oe -o run_dada2.qlog
#PBS -P bart
#PBS -q normal
#PBS -l nodes=1
#PBS -l instance_type=m5.16xlarge
#PBS -l system_metrics=True
#PBS -l scratch_size=25
## END PBS SETTINGS
## BEGIN JOB
```


#### Reading Data from S3

Now we need to make sure our job can get data from S3. Your S3 storage is not available by default from
SOCA, so the first step will be to mount S3 to some location. To do that we mount S3 to some local folder.

For consistency it is probably best for that folder to be in `/scratch/` so that there is no chance
of strange storage costs and to keep our actual locale clean. This also means that you need to make that
folder as part of your job. *Note, Noah tested whether mounting S3 to a folder other than scratch caused any unexpected storage costs and it did not, but this is still probably best practice.*

In this example I mount two locations because I have an AWS profile for bart lab and for datascience and
unfortunately I need data from both and to write to the datascience side (where my main file storage is).

```
# make mountpoints in scratch
mkdir /scratch/mounted_bartlab_data
mkdir /scratch/mounted_datasci

# mount S3
mount-s3 --profile jsumner_bart ddpsc-bartlab-data /scratch/mounted_bartlab_data
mount-s3 --profile jsumner_datasci ddpsc-datascience /scratch/mounted_datasci
```

For the mount to work you will need to configure AWS sso. The first time you do that you will need
to be on the head SOCA node and run `aws configure sso`. That should set up a `.aws` folder in your
home directory where you can then add a `credentials` file per the [usual instructions](https://docs.google.com/document/d/1NdfYnbF7iT6VDSrKsCwh57z5aYbjJHsHGCMBzWVVGyA/edit#heading=h.ok2nu7ilieq7). Note that for some (terrible) reason you will need to refresh your credentials every 12 hours.

To make refreshing your credentials less annoying you might define some alias to open the file in an editor,
but there is currently no getting around that this is a real pain.

#### Running `dada2`

Now we are finally ready to run dada2/whatever our job is. We'll add `Rscript dada2_prep/run_dada2.R`
to our job file and write the R script.

##### Read Quality

One of the inputs that you might frequently want to change would be the truncation length in the
forward and reverse directions.

```{r, eval = FALSE}
plotQualityProfile(c("100ARE_S1_R1_001.fastq.gz", "100ARR_S2_R1_001.fastq.gz")) # forward
plotQualityProfile(c("100ARE_S1_R2_001.fastq.gz", "100ARR_S2_R2_001.fastq.gz")) # reverse
```

![](forward.png)

![](reverse.png)
In these plots we are looking for quality scores of at least 32, shown here.

![](forward_annotated.png)

![](reverse_annotated.png)

Using those plots we can set the truncLength values for our dada2 script. You should check several sequences to inform this choice.

Apart from the truncation lengths you may also want to point to different taxonomic databases (pathToRDP, pathToSILVA, etc) than those shown here, but for SINC projects these are what we have been using.

Those changes would all be made in the first section of the script, the rest should work as is for your data. See comments in the script for details.

```{r, eval = FALSE}
### Load packages
library(parallel)
library(dada2)
library(ShortRead)
library(stringr)
library(seqinr)
print(paste0("Loaded Libraries at ", Sys.time()))
### Specify file paths
#* Note that if you are working on SOCA then you'll need to read
#* data from a mounted S3 bucket. Here the paths correspond to my local
#* so you'd need to change them to match whatever you're doing.
pathToSeq <- "~/scripts/SINC/DARPA/syncom1_rerun/fastqs/"
pathToIntermediateFastq <- "~/scripts/SINC/DARPA/syncom1_rerun/intermediate_fastq"
pathToFilteredFastq <- "~/scripts/SINC/DARPA/syncom1_rerun/filtered"
pathToIndexFile <- "~/scripts/SINC/DARPA/syncom1/"
results_outpath <- "~/scripts/SINC/DARPA/syncom1_rerun/results"
pathToRDP <- "~/scripts/SINC/DARPA/rdp/rdp_train_set_18.fa.gz"
pathToRDPSpec <- "~/scripts/SINC/DARPA/rdp/rdp_species_assignment_18.fa.gz"
### Specify dada2 parameters
truncLength <- c(250, 230)
primer_length <- 20
barcode_length <- 8
rm_len <- primer_length + barcode_length
cores <- 8
save_quality_metrics <- TRUE
temp <- basename(tempdir(check = FALSE))
dir.create(file.path(results_outpath, temp))
### Check some random quality profiles
if (save_quality_metrics) {
  library(ggplot2)
  library(patchwork)
  fastqs <- dir(pathToSeq, pattern = ".fastq.gz", full.names = TRUE)
  index <- sample(seq(1,length(fastqs), 2), 4)
  plot_fastqs <- fastqs[as.vector(rbind(index, index + 1))]
  p1 <- plotQualityProfile(plot_fastqs) +
    facet_wrap(~gsub("L001_|_001.*", "", file))
  p2 <- plotQualityProfile(plot_fastqs, aggregate = TRUE)
  plot <- p1 + p2 + patchwork::plot_layout(axes = "collect") +
    patchwork::plot_annotation(title = pathToSeq) &
    theme(plot.title = element_text(hjust = 0.5))
  ggsave(file.path(results_outpath, temp, "quality_profile.png"),
         plot, device = "png", width = 12, height = 5, dpi = 300)
  # Attempt to do some fastqc quality control stuff.
  library(fastqcr)
  # install fastqc in case it isn't here already
  if (!file.exists("~/bin/FastQC")) {
    tryCatch({fastqc_install()}, error = function(err) {})
  }
  # if the install was successful then use fastqc and make a quality report
  if (file.exists("~/bin/FastQC")) {
    # make plact to put fastqc results
    dir.create(file.path(results_outpath, "fastqc"))
    # run fastqc
    fastqc(fq.dir = pathToSeq,
           qc.dir = file.path(results_outpath, "fastqc"),
           threads = cores)
    # summarize the directory of fastqc results into a data.frame
    qc <- suppressMessages(qc_aggregate(file.path(results_outpath, "fastqc"), progressbar = FALSE))
    qc <- as.data.frame(qc)
    # write the qc data.frame as a csv file
    write.csv(qc, file = file.path(results_outpath, temp, "fastqc_results.csv"), row.names = FALSE)
    # make plot of fastqc stuff
    fqc_plot <- ggplot(qc, aes(fill = status, y = module)) +
      geom_bar() +
      scale_fill_manual(values = c("firebrick4", "gold", "forestgreen"),
                        limits = c("FAIL", "WARN", "PASS")) +
      labs(x = "Count", y = "Module", title = pathToSeq) +
      theme_bw()
    # save plot
    ggsave(file.path(results_outpath, temp, "fasqc_results.png"),
           fqc_plot, device = "png", width = 12, height = 5, dpi = 300)
    # delete the directory
    unlink(file.path(results_outpath, "fastqc"), recursive = TRUE)
  }
}
# read in index hopping control file
InternalIndexes <- readxl::read_excel(file.path(pathToIndexFile, "DARPA_SynCom_barcodes.xlsx"))
colnames(InternalIndexes) <- gsub(" ", "_", colnames(InternalIndexes))
if (!"Internal_Rev_index_RC" %in% colnames(InternalIndexes)) {
  InternalIndexes$Internal_Rev_index_RC <- unlist(lapply(InternalIndexes$Internal_Rev_index, \(x) {
    as.character(ShortRead::reverseComplement(Biostrings::DNAString(x)))
  }))
}
#* Note:
#* it is important that your index hopping file (should come from the sequencer if applicable)
#* has sample names in the same format as the fastq files are named. If that is not the case then
#* you should add some regex here and fix them.

#* get complete list of samples

samps <- unique(sub("_R\\d_.*", "",basename(list.files(pathToSeq, pattern="*.fastq*"))))
print(paste0("Detected ", length(samps), " samples for processing"))

fnFs <- sort(list.files(pathToSeq, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(pathToSeq, pattern="_R2_001.fastq", full.names = TRUE))

#* make names for filtered intermediate fastq files
filtFs <- file.path(pathToFilteredFastq, paste0(samps, "_F_filt.fastq.gz"))
filtRs <- file.path(pathToFilteredFastq, paste0(samps, "_R_filt.fastq.gz"))
names(filtFs) <- samps
names(filtRs) <- samps

#* separate sequences with an entry in `InternalIndexes$Sample_ID`

formatted_samps_names <- gsub("_S.*", "", samps) # format samps_22 to match Sample_ID

fnFs_with_barcodes <- fnFs[which(formatted_samps_names %in% InternalIndexes$Sample_ID)]
fnRs_with_barcodes <- fnRs[which(formatted_samps_names %in% InternalIndexes$Sample_ID)]
filtFs_with_barcodes <- filtFs[which(formatted_samps_names %in% InternalIndexes$Sample_ID)]
filtRs_with_barcodes <- filtRs[which(formatted_samps_names %in% InternalIndexes$Sample_ID)]
fnFs_skipping_index_hopping <- fnFs[which(!formatted_samps_names %in% InternalIndexes$Sample_ID)]
fnRs_skipping_index_hopping <- fnRs[which(!formatted_samps_names %in% InternalIndexes$Sample_ID)]

if (length(fnFs_skipping_index_hopping) > 0 ){
  warning("Some sequences not covered by Internal Index File, compare samps vs fnFs")
}

#* ***** `Index Hopping/Primer Trimming`

intermediate_fastqs <- mclapply(seq_along(fnFs_with_barcodes), function(i) {
  fwd_file <- fnFs_with_barcodes[i]
  rev_file <- fnRs_with_barcodes[i]
  internal <- InternalIndexes[InternalIndexes$Sample_ID == gsub("_S.*", "", basename(fwd_file)), ]
  fwd <- ShortRead::readFastq(fwd_file)
  fwd_df <- data.frame(seqs = as.character(fwd@sread))
  fwd_df$quality <- as.character(fwd@quality@quality)
  fwd_df$ids <- as.character(fwd@id)
  fwd_df$index <- seq_len(nrow(fwd_df))
  fwd_df$fwd_barcode <- substr(fwd_df$seqs, 1, barcode_length)
  fwd_df$rev_barcode <- substr(fwd_df$seqs, nchar(fwd_df$seqs) - barcode_length - 1, nchar(fwd_df$seqs))
  fwd_df$seqs <- substr(fwd_df$seqs, (rm_len) + 1, nchar(fwd_df$seqs))
  fwd_df$quality <- substr(fwd_df$quality, (rm_len) + 1, nchar(fwd_df$quality))
  fwd_indices <- fwd_df[fwd_df$fwd_barcode == internal$Internal_Fwd_index, "index"]
  rev <- ShortRead::readFastq(rev_file)
  rev_df <- data.frame(seqs = as.character(rev@sread))
  rev_df$quality <- as.character(rev@quality@quality)
  rev_df$ids <- as.character(rev@id)
  rev_df$index <- seq_len(nrow(rev_df))
  rev_df$fwd_barcode <- substr(rev_df$seqs, 1, barcode_length)
  rev_df$rev_barcode <- substr(rev_df$seqs, nchar(rev_df$seqs) - barcode_length - 1, nchar(rev_df$seqs))
  rev_df$seqs <- substr(rev_df$seqs, (rm_len) + 1, nchar(rev_df$seqs))
  rev_df$quality <- substr(rev_df$quality, (rm_len) + 1, nchar(rev_df$quality))
  rev_indices <- rev_df[rev_df$fwd_barcode == internal$Internal_Rev_index_RC, "index"]
  both_barcodes_present <- intersect(fwd_indices, rev_indices)
  fwd_out <- ShortRead::ShortReadQ(sread = Biostrings::DNAStringSet(fwd_df[both_barcodes_present, "seqs"]),
                                   id = Biostrings::BStringSet(fwd_df[both_barcodes_present, "ids"]),
                                   quality = Biostrings::BStringSet(fwd_df[both_barcodes_present, "quality"]))
  rev_out <- ShortRead::ShortReadQ(sread = Biostrings::DNAStringSet(rev_df[both_barcodes_present, "seqs"]),
                                   id = Biostrings::BStringSet(rev_df[both_barcodes_present, "ids"]),
                                   quality = Biostrings::BStringSet(rev_df[both_barcodes_present, "quality"]))
  fwd_out_path <- file.path(pathToIntermediateFastq, basename(fwd_file))
  rev_out_path <- file.path(pathToIntermediateFastq, basename(rev_file))
  ShortRead::writeFastq(fwd_out, file = fwd_out_path)
  ShortRead::writeFastq(rev_out, file = rev_out_path)
  message(
    paste0(
      "Read Sample ", basename(fwd_file), " containing ", nrow(fwd_df), " (", nrow(rev_df), ")",
      " reads and writing with ", length(both_barcodes_present), " reads having removed index hopping"
    )
  )
  return(list("fwd" = fwd_out_path, "rev" = rev_out_path,
              "n_fwd" = nrow(fwd_df), "n_rev" = nrow(rev_df),
              "fwd_matches" = length(fwd_indices),
              "rev_matches" = length(rev_indices),
              "kept" = length(both_barcodes_present))
  ) # write out metadata about removal amount
}, mc.cores = cores)
index_hopping_df <- do.call(rbind, lapply(intermediate_fastqs, as.data.frame))

fnFs_barcode_rm <- unlist(lapply(intermediate_fastqs, function(x) x$fwd))
fnRs_barcode_rm <- unlist(lapply(intermediate_fastqs, function(x) x$rev))
names(fnFs_barcode_rm) <- names(filtFs_with_barcodes)
names(fnRs_barcode_rm) <- names(filtRs_with_barcodes)

print(paste0("Filtering and trimming barcoded reads at ", Sys.time()))

out <- matrix(c(NA, NA), nrow = 1)
out <- tryCatch({ # filter and Trim removed all reads here. Maybe use plotQualityProfile on filtered?
  filterAndTrim(fnFs_barcode_rm, filtFs_with_barcodes, fnRs_barcode_rm, filtRs_with_barcodes,
                truncLen=truncLength, trimLeft=0, trimRight=0, maxN=0, maxEE=c(2,2),
                truncQ=2, rm.phix=TRUE,
                compress=TRUE, multithread = cores, n = 1000)
}, error = function(err){
  message("Error caught during filterAndTrim, it is likely that you ran out of storage or have archived data: ")
  message(conditionMessage(err))
  matrix(c(NA, NA), nrow = 1)
})

print(paste0("Checking successfully filtered sequences at ", Sys.time()))
filtFs_attempted <- filtFs
filtRs_attempted <- filtRs
filtFs <- filtFs[file.exists(filtFs)]
filtRs <- filtRs[file.exists(filtRs)]
if(length(filtFs) != length(filtFs_attempted)) {
  warning(paste0("NOT ALL FORWARD SEQUENCES SUCCEEDED IN FILTERING STEP, EXPECTED ", 
                 length(filtFs_attempted), " BUT ONLY HAVE ", length(filtFs)))
}
if(length(filtRs) != length(filtRs_attempted)) {
  warning(paste0("NOT ALL REVERSE SEQUENCES SUCCEEDED IN FILTERING STEP, EXPECTED ", 
                 length(filtRs_attempted), " BUT ONLY HAVE ", length(filtRs)))
}

### Learning error rates
loessErrfun_mod <- function (trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow = 0, ncol = length(qq))
  for (nti in c("A", "C", "G", "T")) {
    for (ntj in c("A", "C", "G", "T")) {
      if (nti != ntj) {
        errs <- trans[paste0(nti, "2", ntj), ]
        tot <- colSums(trans[paste0(nti, "2", c("A",
                                                "C", "G", "T")), ])
        rlogp <- log10((errs + 1)/tot)
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q = qq, errs = errs, tot = tot,
                         rlogp = rlogp)
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)
        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred) > maxrli] <- pred[[maxrli]]
        pred[seq_along(pred) < minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      }
    }
  }
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-07
  est[est > MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est < MIN_ERROR_RATE] <- MIN_ERROR_RATE
  err <- rbind(1 - colSums(est[1:3, ]), est[1:3, ], est[4,
  ], 1 - colSums(est[4:6, ]), est[5:6, ], est[7:8, ], 1 -
    colSums(est[7:9, ]), est[9, ], est[10:12, ], 1 - colSums(est[10:12,
    ]))
  rownames(err) <- paste0(rep(c("A", "C", "G", "T"), each = 4),
                          "2", c("A", "C", "G", "T"))
  colnames(err) <- colnames(trans)
  return(err)
}

print(paste0("Learning error rates at ", Sys.time()))
errF = learnErrors(filtFs_with_barcodes, multithread = cores, errorEstimationFunction = loessErrfun_mod)
errR = learnErrors(filtRs_with_barcodes, multithread = cores, errorEstimationFunction = loessErrfun_mod)

### Dereplication

print(paste0("Dereplicating Fastq files ", Sys.time()))
derepFs <- derepFastq(filtFs_with_barcodes, verbose = FALSE)
derepRs <- derepFastq(filtRs_with_barcodes, verbose = FALSE)

### Denoising to Amplicon Sequence Variants

print(paste0("Denoising to unique sequences at ", Sys.time()))
dadaFs <- dada(derepFs, err=errF, multithread=cores)
dadaRs <- dada(derepRs, err=errR, multithread=cores)

### Merging reads

print(paste0("Merging reads at ", Sys.time()))
mergers <- mergePairs(dadaFs, filtFs_with_barcodes,
                      dadaRs, filtRs_with_barcodes)

### Generate ASV table
print(paste0("Generating ASV table at ", Sys.time()))
seqtab <- makeSequenceTable(mergers)

### Removing Chimeras
print(paste0("Removing Chimeras at ", Sys.time()))
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=cores)
seqtab.print <- seqtab.nochim
colnames(seqtab.print) <- paste0("ASV",1:length(colnames(seqtab.print)))

### Track sequences
print(paste0("Tracking sequences at ", Sys.time()))
getN <- function(x) sum(dada2::getUniques(x))
track <- cbind(sapply(dadaFs, getN), sapply(dadaRs, getN),
               sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("denoisedF", "denoisedR", "merged","nonchim")
tryCatch({
  rownames(out) <- gsub("_R._001.fastq.gz", "", rownames(out))
  out <- out[names(dadaFs), ]
  track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),
                 sapply(mergers, getN), rowSums(seqtab.nochim))
  colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged","nonchim")
}, error = function(err){
  message("Had an error making tracking object")
  message(conditionMessage(err))
})

### Generate fasta file
print(paste0("Generating fasta file at ", Sys.time()))
asvseq <- colnames(seqtab.nochim)
write.fasta(sequences = lapply(asvseq,identity),names=paste0("ASV",1:length(asvseq)),
            file.out = file.path(results_outpath, temp, "dada2.fasta"))

### Save intermediate rdata file
asvseq_to_number <- data.frame(
  seq = colnames(seqtab.nochim),
  number = colnames(seqtab.print)
)
write.csv(asvseq_to_number, file = file.path(results_outpath, temp, "seq_to_number.csv"))
write.csv(seqtab.print, file = file.path(results_outpath, temp, "seqtab_print.csv"))
write.csv(track, file = file.path(results_outpath, temp, "tracking.csv"))
write.csv(index_hopping_df, file = file.path(results_outpath, temp, "index_hopping.csv"))

### Assign taxonomy
print(paste0("Assigning taxonomy at ", Sys.time()))
taxa_rdp <- assignTaxonomy(seqtab.nochim, pathToRDP, multithread=cores)
rownames(taxa_rdp) <- paste0("ASV",1:length(asvseq))

write.csv(taxa_rdp, file = file.path(results_outpath, temp, "taxa_rdp.csv"))

print(paste0("Adding Species Level taxonomy at ", Sys.time()))
rownames(taxa_rdp) <- colnames(seqtab.nochim)
taxa_rdp <- addSpecies(taxa_rdp, pathToRDPSpec)
rownames(taxa_rdp) <- paste0("ASV",1:length(asvseq))

write.csv(taxa_rdp, file = file.path(results_outpath, temp, "taxa_rdp.csv"))

print(paste0("Zipping Output at", Sys.time()))
# zip everything up
system(paste0("zip -j ", file.path(results_outpath, "dada2_output.zip"), " ",
              file.path(results_outpath, temp, "*")))
# if the zip file exists then delete the temp directory
if (file.exists(file.path(results_outpath, "dada2_output.zip"))) {
  unlink(file.path(results_outpath, temp), recursive = TRUE)
}

print(paste0("Done at ", Sys.time()))
```

#### Saving data

If you are confident in your ability to keep the credentials up to date then this is very easy and you
will just copy the data from `/scratch/yourdata` to
`/scratch/mountpoint/wherever/it/goes/next`.

For an extra layer of protection we can wrap that part of our job in an if/else statement in case something
has gone wrong and the mountpoint is no longer connected. Here we check that the mountpoint contains some
directory that exists on S3. If that directory is found then we make the target destination and write our
output to it. If the directory did not exist then we write the data to local storage on the soca head node.

Note that if you write data to the local head node on soca it will be very expensive to store it there
long term and you should make sure to check soon if you need to move something in an interactive job
from soca to S3 (ie, qsub -I job.sh, mount S3, copy from local to S3, check that it is in S3, delete the 
local copy). Note that here I broke up the `cp` lines so that the entire thing is within margins, but
you should have those each as one line.

```
# if the mounted directory exists then
if [ -d /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/ ]; then
   # write to S3
   cp /scratch/run_dada2_results.rdata
      /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/
   mkdir  /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/dada2_soca/
   cp /scratch/run_dada2_results.rdata
      /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/dada2_soca/
else  # if the mount is not there then
   # write to EBS storage
   cp /scratch/dada2_output.zip /shares/datascience/users/jsumner/dada2_prep/temp_storage/.
fi
```

#### Job Cleanup

It is probably not very important, but it's nice to unmount S3 from wherever you put it. 

```
fusermount -u /scratch/mounted_bartlab_data
fusermount -u /scratch/mounted_datasci
```

### Complete Job File

So your complete job file would look like this, with the R script defined above.

```
#!/bin/bash
## BEGIN PBS SETTINGS: Note PBS lines MUST start with #
#PBS -N run_dada2
#PBS -V -j oe -o run_dada2.qlog
#PBS -P bart
#PBS -q normal
#PBS -l nodes=1
#PBS -l instance_type=m5.24xlarge
#PBS -l system_metrics=True
#PBS -l scratch_size=200
## END PBS SETTINGS
## BEGIN JOB

# make mountpoints in scratch
mkdir /scratch/mounted_bartlab_data
mkdir /scratch/mounted_datasci

# mount S3
mount-s3 --profile jsumner_bart ddpsc-bartlab-data /scratch/mounted_bartlab_data
mount-s3 --profile jsumner_datasci ddpsc-datascience /scratch/mounted_datasci

# Copy taxa databases to scratch for ease of use
cp /scratch/mounted_datasci/shares/datascience/users/jsumner/microbiome/Taxa/rdp_train_set_18.fa.gz
   /scratch/rdp_train_set_18.fa.gz
cp /scratch/mounted_datasci/shares/datascience/users/jsumner/microbiome/Taxa/rdp_species_assignment_18_withBart.fa.gz /scratch/rdp_species_assignment_18_withBart.fa.gz

Rscript dada2_prep/run_dada2.R

# if the mounted directory exists then
if [ -d /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/ ]; then
   # write to S3
   cp /scratch/run_dada2_results.rdata /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/
   mkdir  /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/dada2_soca/
   cp /scratch/dada2_output.zip
      /scratch/mounted_datasci/shares/datascience/users/jsumner/SINC/dada2_soca/
else  # if the mount is not there then
   # write to EBS storage
   cp /scratch/dada2_output.zip /shares/datascience/users/jsumner/dada2_prep/temp_storage/.
fi

fusermount -u /scratch/mounted_bartlab_data
fusermount -u /scratch/mounted_datasci

## END JOB

```

### Miscellaneous Errors

#### Archived S3 Objects

You may get an error copying the taxonomy files to `/scratch/`. If it is an input/output error then the problem is most likely that the data is archived. You will need to recover the data.

```
[jsumner@ip-10-0-77-133 dada2_prep]$ cp /scratch/mounted_datasci/shares/datascience/users/jsumner/microbiome/Taxa/rdp_species_assignment_18_withBart.fa.gz
/scratch/.
cp: error reading ‘/scratch/mounted_datasci/shares/datascience/users/jsumner/microbiome/Taxa/rdp_species_assignment_18_withBart.fa.gz’: Input/output error
cp: failed to extend ‘/scratch/./rdp_species_assignment_18_withBart.fa.gz’: Input/output error
```

In R this would show up as an error like this:

```
Error in tax[[1]] : subscript out of bounds
Calls: assignTaxonomy -> grepl -> is.factor
Execution halted
```

#### Running out of scratch memory

If you run out of scratch memory then you may see messages like this:

```
[1] "Filtering and trimming"
Creating output directory: /scratch/filtered
Error in filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = truncLength,  : 
  These are the errors (up to 5) encountered in individual cores...
Error in writeFastq(fqF, fout[[1]], "w", compress = compress) : 
  failed to write record 3937
Error in writeFastq(fqR, fout[[2]], "w", compress = compress) : 
  failed to write record 12722
Error in writeFastq(fqF, fout[[1]], "w", compress = compress) : 
  failed to write record 286
Error in writeFastq(fqF, fout[[1]], "w", compress = compress) : 
  failed to write record 315
Error in writeFastq(fqF, fout[[1]], "w", compress = compress) : 
  failed to write record 2482
In addition: Warning message:
In mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule,  :
  all scheduled cores encountered errors in user code
Execution halted
```

You might also get more esoteric error messages, like this one that just says the process was killed.
This happened after assigning taxonomy but before being done, so it probably has to do with writing out
the rdata file. After this I went from a c5.24xlarge machine to an m5.24xlarge machine and upped the
scratch storage from 25Gb to 100Gb. I do not know which of those was necessary, but scratch seems like
the most likely candidate.

```
/var/spool/pbs/mom_priv/jobs/18146.ip-10.SC: line 26:  5623 Killed Rscript dada2_prep/run_dada2.R
```


In this case, request more scratch memory.

#### Running out of memory

If you run out of machine memory then you will see errors about not being able to assign objects.
In that case you need a larger instance.


## Tracking DADA2

When the job is done running you can check the progress using the `track` object in the `dada2_output` rdata (which is the same as the `dada2_sequence_tracking.csv` file). That will show you where your data are being filtered the most and allow you to compare trends between experiments if that is of interest.

```{r, eval = TRUE, echo = FALSE}
track <- structure(list(
  id = c(
    1L, 2L, 3L, 4L, 5L, 6L, 1L, 2L, 3L, 4L,
    5L, 6L, 1L, 2L, 3L, 4L, 5L, 6L, 1L, 2L, 3L, 4L, 5L, 6L, 1L, 2L,
    3L, 4L, 5L, 6L
  ), sample = c(
    "100ARE", "100ARR", "100AS", "100BRE",
    "100BRR", "100BS", "100ARE", "100ARR", "100AS", "100BRE", "100BRR",
    "100BS", "100ARE", "100ARR", "100AS", "100BRE", "100BRR", "100BS",
    "100ARE", "100ARR", "100AS", "100BRE", "100BRR", "100BS", "100ARE",
    "100ARR", "100AS", "100BRE", "100BRR", "100BS"
  ), tissue = c(
    "ARE",
    "ARR", "AS", "BRE", "BRR", "BS", "ARE", "ARR", "AS", "BRE", "BRR",
    "BS", "ARE", "ARR", "AS", "BRE", "BRR", "BS", "ARE", "ARR", "AS",
    "BRE", "BRR", "BS", "ARE", "ARR", "AS", "BRE", "BRR", "BS"
  ),
  depth = c(
    "A", "A", "A", "B", "B", "B", "A", "A", "A", "B",
    "B", "B", "A", "A", "A", "B", "B", "B", "A", "A", "A", "B",
    "B", "B", "A", "A", "A", "B", "B", "B"
  ), tiss = c(
    "RE", "RR",
    "S", "RE", "RR", "S", "RE", "RR", "S", "RE", "RR", "S", "RE",
    "RR", "S", "RE", "RR", "S", "RE", "RR", "S", "RE", "RR",
    "S", "RE", "RR", "S", "RE", "RR", "S"
  ), variable = structure(c(
    1L,
    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L,
    3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L
  ), levels = c(
    "input",
    "filtered", "denoisedF", "denoisedR", "merged", "nonchim"
  ), class = "factor"), value = c(
    21960, 37723, 40306, 30558,
    47596, 39143, 15803, 28054, 28611, 21285, 34732, 27402, 15461,
    24742, 24445, 21086, 30951, 23604, 15494, 26192, 27039, 21141,
    32609, 25535, 8388, 14802, 12895, 9303, 19101, 13816
  ), name = structure(c(
    1L,
    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L,
    3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L
  ), levels = c(
    "input",
    "filtered", "denoisedF", "denoisedR", "merged", "nonchim"
  ), class = c("ordered", "factor"))
), row.names = c(NA, 30L), class = c("data.frame"))
```


```{r, eval=TRUE}
ggplot(track, aes(x = variable, y = value, group = id, color = interaction(depth, tiss))) +
  facet_grid(depth ~ tiss, scale = "free") +
  geom_line(alpha = 0.75, linewidth = 0.25, show.legend = FALSE) +
  labs(y = "Reads", title = "DADA2 - Reads") +
  theme_minimal() +
  theme(
    axis.text.x.bottom = element_text(angle = -45, hjust = 0),
    axis.title.x.bottom = element_blank(),
    strip.background = element_rect(fill = "gray50", color = "gray20"),
    strip.text.x = element_text(size = 14, color = "white"),
    strip.text.y = element_text(size = 14, color = "white")
  )
```
